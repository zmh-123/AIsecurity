# 计算机安全速记版

## 第一讲 绪论

1. **韦恩图**

​	<img src="D:\大三下\计算机安全\AIsecurity\image-20250702110835409.png" alt="image-20250702110835409.png" style="zoom:70%;" />

2. **什么是人工智能**

   人工智能（Artificial Intelligence，简称 AI）是一门研究如何让机器模拟人类智能行为的科学与技术，其核心目标是让机器具备类似人类的感知、学习、推理、决策甚至创造等能力。

3. **机器学习中有哪些模型**

   线性回归、逻辑回归、决策树、卷积神经网络（CNN）、循环神经网络（RNN）

4. **AI安全分为什么**

   ![image-20250702111516344.png](D:\大三下\计算机安全\AIsecurity\image-20250702111516344.png)

## 第二讲 对抗样本

1. **什么是对抗样本**

   举个例子，特斯拉在识别交通标志的时候被对抗样本攻击导致加速的案例引入（Researchers Tape Speed Limit Sign to Make Teslas Accelerate to 85 MPH），原本识别为“35英里限速”标志，会被误识别，并采取错误的控制行为（加速到85英里）。

   由于细微的对抗噪声和最差情况的恶意攻击，导致输入样本被错误分类。

   **与投毒样本不同，** 对抗样本不是更改模型的训练集。它做的是修改输入模型的数据

2. **模型失效的原因**

   对抗样本，由于细微的对抗噪声和最差情况的恶意攻击，导致输入样本被错误分类

   领域偏移，现实场景的数据与训练时的数据不一致，导致模型的泛化能力下降。

   未知数据，数据不在模型训练的数据范围内。

3. **什么是数据投毒**

   数据投毒又被叫做Causative Attack (原因性攻击) 。是指通过修改一部分训练数据使得模型做出错误的判断。通常是想影响模型后期的表现，例如更改模型的权重，偏差或者参数。

   我们以微软小冰为案例， 在互联网早期，网络社区涌现出大量不良信息,有些被别有用心的人利用来“教导”微软小冰，导致对话变得不合适。

4. **对抗样本之所以存在，是不是因为模型存在过拟合现象呢？**

   不完全是。虽然过拟合可能使模型更脆弱，但研究表明，即使是线性模型，也存在对抗样本。其根本原因在于，模型（尤其是高维空间中的模型）**感知数据的方式与人眼不同**。模型可能会依赖一些与我们认知无关但统计上相关的微弱特征，而攻击者正是利用了这一点。

5. **攻击分类**

   白盒攻击：在白盒场景下，攻击者可以清楚地看到模型的内部结构和参数，就像一个透明的盒子。因此，他可以计算损失函数对输入的梯度（$∇*L$），从而精确地找到最有效的攻击方向。

   黑盒攻击：在黑盒场景下，模型对攻击者来说是一个不透明的黑箱。他无法直接计算梯度，只能通过不断地查询模型（输入数据，观察输出）来猜测模型的行为，攻击难度更大。

6. **如何构造一个对抗样本**

   我们希望找到一个修改后的样本，它与原始样本非常接近，但能让模型做出错误的分类。

   - 基于黑盒模型

     攻击者自己训练一个和目标模型功能相似的模型（比如，如果目标是图像分类，攻击者也训练一个图像分类模型）。因为攻击者对自己的替代模型有完全的控制权，所以这是一个白盒环境。攻击者在自己的替代模型上生成对抗样本。将这些对抗样本输入到目标黑盒模型中。由于迁移性的存在，这些样本有很大概率也能欺骗目标模型。

7. **经典的白盒攻击算法（快速符号梯度法）**

   沿着损失函数对输入x的梯度方向，迈出一小步。梯度方向是函数值增长最快的方向，所以沿着这个方向修改输入，最容易让损失变大。

8. **防御方法概述**

   1. **数据预处理:** 在将数据喂给模型前，先尝试去除对抗性噪声。
   2. **模型加固 :** 修改模型结构或训练过程，使其更具鲁棒性。
   3. **检测:** 在分类前，先检测输入是否为对抗样本。

9. **对抗训练**

   在训练过程中，不断地生成对抗样本，并把这些“难题”也加入到训练集中，教会模型如何正确地分类它们。

   用数学形式表达是一个min-max优化问题。内部循环（max）是在寻找最能让模型犯错的对抗样本，外部循环（min）是在调整模型参数以正确分类这些难题。（这里像是训练和攻击的对偶性不过那个是反过来了是为了攻击这个是为了防御）

10. **如何评估防御的好坏**

    1. **明确威胁模型:** 你的防御是针对什么级别的攻击者？白盒还是黑盒？扰动多大？
    2. **公开透明:** 公开你的代码和模型，让社区来检验。
    3. **强力评估:** 使用自适应攻击（专门针对你的防御设计的攻击）进行测试，而不仅仅是已知的标准攻击。

## 第三到四讲 模型后门&数据投毒

1. **实验一**

   把狼识别为苹果，就是一个典型的后门攻击场景

2. **攻击分类**

   - 攻击时机

     投毒攻击：发生在训练阶段，攻击者污染训练数据或训练过程。

     规避攻击：测试阶段，就像对抗样本一样，攻击者修改输入来欺骗一个已经训练好的模型。

   - 攻击目标

     有目标攻击就是指定分类类别，无目标攻击就是分错就行。

3. **后门攻击**

   加入一个触发器（trigger），只有当触发器出现时，后门才会被激活；否则，模型的行为一切正常，这使得后门非常隐蔽。

   例如，攻击者在训练数据中混入了一些带有特殊标记（比如一个黄色小贴纸）的“停止”路牌，但却把这些图片的标签错误地标为“限速65”。

4. **后门攻击分类速记**

   **外包 (Outsourcing):** 把模型训练外包给不怀好意的第三方。

   **预训练 (Pretrained):** 下载一个已经被植入后门的预训练模型。

   **数据收集 (Data collection):** 从网上爬取的数据本身就含有毒数据。

   **协同学习 (Collaborative learning):** 在联邦学习中，恶意参与方上传有毒的更新。

   **部署后 (Post-deployment):** 模型部署后被黑客篡改。

   **代码投毒 (Code poisoning):** 使用了含有恶意代码的开源训练框架。

   **核心关系 (Slide 9):** “数据投毒是实现后门攻击的主要手段”。

5. **后门攻击有什么好处**

   可以用在保护知识产权。比如**模型水印**，通过植入一个只有模型所有者知道的特定后门，可以用来证明模型的知识产权。

6. **如何防御后门攻击**

   - 检测后门

     检测被投毒的图像在频域上是否异常

   - 后门移除

     一是模型分析，逆向工程，尝试为每个类别恢复出可能存在的最小触发器。如果某个类别的触发器异常地小，就说明该类别可能被植入了后门。二是可以通过**ABS**神经元分析，识别出哪些神经元对后门触发器“过度兴奋”，并进行修复。

## 第五讲 模型逆向

这里就到了AI机密性的范畴了

1. **为什么需要模型提取**

   贵 very

   如果有人能通过某种手段，以很低的成本“复制”一个功能几乎一模一样的模型，这无疑就是一种“窃取”行为。

2. **模型提取的攻击目标**

   攻击者希望通过尽可能少的查询，学习到一个目标模型 $f$ 的一个**近似模型 $f'$**。这个近似要到什么程度呢？比如，在99.9%的输入上，$f'$的输出和$f$完全一样。

3. **为什么这不是普通的机器学习过程**

   关键在于，商业化的预测API通常会返回**比简单的类别标签更多的信息**，比如**置信度分数 **。这些额外的“软标签”信息泄露了更多关于模型决策边界的秘密，使得攻击者能用更少的数据学到更好的模型。

4. **针对简单模型的方程求解攻击（这里以逻辑回归为例）**

   - 逻辑回归的输出 f(x) 是一个概率值，其公式可以变换成一个关于未知参数 w 和 b 的线性方程。
   - 在这个方程中，w和b是我们要窃取的目标，总共有 d+1 个未知数（d是特征维度）。
   - **攻击算法:** 根据线性代数的知识，要解 d+1 个未知数，我们只需要构造一个包含 d+1 个方程的线性方程组。具体做法就是：我们**查询 d+1 个不同的输入点 x**，得到对应的输出 f(x)，然后代入方程，就能解出 w 和 b。这样就完美地提取了整个模型！

5. **如何从逻辑回归攻击引入到通用方程求解攻击**

   - 一个有 k 个参数（权重W）的模型，其输入输出关系可以看作是一个复杂的非线性方程。
   - 攻击者可以随机生成一些输入X，查询得到输出Y（包含置信度）。
   - 然后，攻击者通过优化算法（如梯度下降）来寻找一组自己的模型参数W，使得自己的模型f'(x; W)的输出与查询到的f(x)的输出尽可能接近。
   - **效果惊人:** 这种方法可以达到 >99% 的一致性，并且平均每个未知权重只需要约1次查询！

6. **模型提取应用到增强型模型反演攻击**

   先提取模型在进行模型反演查询输入，总体来看可以大幅降低查询成本（提取完模型之后后续查询不用掏钱）。

7. **提取决策树**

   因离散，不能直接解方程。

   根据置信度不同来进行提取，如果查询x和x'得到了不同的置信度，说明模型在这两个输入上走到了不同的叶子节点，从而证明了模型是根据这个差异特征进行“分裂 ”的。

8. **防御模型提取的措施**

   API最小化、不返回置信度、对置信度进行处理

9. **攻击者的应对**

   利用**主动学习** 的思想，优先查询那些靠近当前学到的决策边界的点，从而高效地更新和优化替代模型。虽然效率比解方程低（约100倍），但仍然是可行的。

## 第六讲 数据窃取

1. **链接攻击**

   攻击者通过将“匿名化”的数据集与另一个公开可用的数据集进行链接，从而恢复出个体的身份。

2. **K-匿名防御**

   这是一种更强的匿名化技术，它要求数据集中的每一条记录，都无法与另外 k-1 条记录区分开。通过对准标识符进行泛化（比如把具体年龄变成年龄段）或抑制（隐藏某些信息），来实现这个目标。

   局限性就是无法抵御同质性攻击（如果一个K匿名组内的所有人都患有同一种疾病，那攻击者虽然不知道具体是谁，但能100%确定这个组里的某个人（比如Bob）患有心脏病。）和背景知识攻击，并且可能会过度隐藏信息，导致数据可用性大大降低。

3. **数据提取（模型会泄露信息）**

   既然模型是从数据中学来的，那么它必然在某种程度上“记住”了这些数据。数据提取攻击就是**通过模型逆向得到训练数据**。

4. **DNN记忆数据的证据**

   - **证据1：分层表示 :** 深度神经网络（DNN）学习的是从低级到高级的分层特征，这些特征本身就是对数据的一种抽象和记忆。
   - **证据2：记忆随机标签 :** 实验表明，DNN有强大的能力去“死记硬背”完全随机的标签或像素。这证明了它的记忆能力远超我们的想象，甚至可以过拟合到纯粹的噪声。
   - **证据3：生成模型的成功 :** 像GAN和扩散模型这样的生成模型，能够创造出以假乱真的图像（比如 thispersondoesnotexist.com），这本身就证明了模型已经深入地“记忆”和“理解”了训练数据的分布和细节。

5. **为什么DNN会记忆数据**

   模型参数远远大于训练样本数，使其有足够的“空间”去记住数据。

   最小化损失函数的目标，会驱使模型去过拟合那些噪声或异常样本，因为“记住”它们是降低整体损失的最快方式。

   训练数据中的噪声、重复样本或罕见但敏感的样本，更容易被模型记住。

6. **数据窃取攻击的分类和方法**

   数据窃取攻击可以大致分为黑盒和白盒两类。黑盒攻击又可以细分为“已知窃取数据”和“成员推理”等。

7. **如何实现模型反演攻击**

   即从一个训练好的模型中，恢复出能代表某个类别的典型训练样本。在人脸识别中，就是恢复出某个人的面部图像。

   基于梯度下降：

   1. 从一个随机噪声图像开始。
   2. 不断地调整这个图像的像素，目标是让模型在看到这张图像时，将其识别为目标人物的**置信度最高**。
   3. 这个过程本质上是在梯度引导下，寻找那个最能“激活”目标神经元的输入图像。

   攻击时间长，一个简单的防御方法就是降低返回的置信度的精度。

8. **提取具体训练样本**

   比如一个邮件自动补全的语言模型，在训练时看到过包含“Zizhuang's ID No. is 123-45-6789”的邮件。当用户输入“Zizhuang's ID No. is”时，模型可能会直接补全出那个被记住的ID号。

   攻击者可以穷举所有可能的敏感信息（比如所有可能的社保号），然后计算模型生成这些信息的概率。如果某个特定信息的生成概率**异常地高**，那么这个信息很大概率就存在于训练数据中。

   但查询量太大，可用Dijkstra最短路径搜索的思想，可以将查询次数降低好几个数量级。

9. **如何选择安全的模型**

   模型A准确率高（96%），但记忆性也强；模型B准确率稍低（92%），但几乎不记忆数据。选择哪一个，这涉及到准确性和隐私性的权衡。

   - 基于暴露度的测试方法

     如果一个模型能记住我们故意插入的、完全随机的“金丝雀 (canary)”数据（比如一串无意义的单词），那么它很可能也记住了其他真实的训练数据。

     1. 生成一个“金丝雀”假数据。
     2. 将其插入到训练数据中。
     3. 训练模型。
     4. 计算这个“金丝雀”的**暴露度**（模型生成金丝雀的概率 / 模型生成其他普通样本的期望概率），如果这个比值远大于1，说明模型对这个金丝雀“情有独钟”，也就是记住了它。

     画出模型的“效用-暴露度”图。横轴是模型在测试集上的性能（效用），纵轴是金丝雀的暴露度。我们可以选择那些在效用相近的情况下，暴露度尽可能低的（图上橙色曲线的拐点附近）模型。

10. **总结结论**

    无意记忆的发生**不依赖于过拟合**，它是一种普遍现象。

    罕见的数据**在模型达到最佳性能之前**就可能被记住。

    常规的正则化方法（如权重衰减、dropout、量化）**不能**解决记忆问题。

    **差分隐私 (Differential Privacy)** 是目前唯一能从理论上提供隐私保护的方法，但它通常会以牺牲模型性能为代价。

## 第七讲 成员推理（MIA）

1. **为什么成员推理攻击很敏感**

   即判断**某个特定的数据点是否在模型的训练集中**。

   举例来说，如果一个用于诊断某种“难以启齿的疾病”的模型被攻击，判断出某人的数据在训练集中，就相当于泄露了这个人患有此病的隐私。

2. **MIA的核心思想**

   攻击者需要训练一个**二分类模型**，来学习区分“**在训练集中的样本（in）**”和“**不在训练集中的样本（out）**”在经过目标模型处理后，其行为模式有什么不同。

3. **影子模型**

   构建训练集D（随机合成/基于统计的合成/带噪声的训练数据）

   从D中取k个不相交的子集训练k个影子模型

   用训练集（in-data）和测试集（out-data）输入对应影子模型，得到预测结果（置信度向量）

   > 举个例子：
   >
   > - in 样本（它训练过的苹果图）：影子模型 1 预测 “是苹果” 的概率很高（比如 95%）；
   > - out 样本（它没训练过的苹果图）：影子模型 1 预测 “是苹果” 的概率较低（比如 60%）。

   把（预测结果，"in"）和（预测结果，"out"）这样的数据对，组合成我们最终用来训练攻击模型的数据集。

   有了攻击数据集，我们就可以训练一个分类器，它的输入是目标模型的预测向量，输出是“in”或“out”的概率。

   > 如果模型对一张苹果图的预测概率很高（比如 > 90%），大概率是它训练过的 in 样本；如果概率较低（比如 < 70%），大概率是没训练过的 out 样本”。

4. **传统MIA的局限**

   模型必须过拟合（核心依赖）

5. **指标指导的MIA**

   - **预测正确性:** 模型对训练集成员的预测通常更可能正确。
   - **预测损失:** 成员的损失值通常低于非成员。
   - **预测置信度:** 成员的置信度通常更高。
   - **预测熵:** 成员的预测概率分布通常更“尖锐”，即熵更低。

## 第八到九讲 能量延迟

这一讲主要是针对AI的可用性。

1. **海绵样本攻击**

   攻击者构造一种特殊的输入，我们称之为“海绵样本”。这种样本输入到模型后，会像海绵吸水一样，疯狂地消耗计算资源，导致延迟增加和能量过耗。

2. **能量延迟攻击的原理**

   构造一种输入，使得模型在处理它时，所需的运算量或放存量急剧增加。

3. **如何寻找海绵样本**

   随机样本池——输入模型测量适应度（延迟时间或者能量消耗）——取最高适应度的样本作为父代——对父代进行变异和交叉产生子代——重复这个过程产生毒海绵样本

   另外一种是白盒优化，希望找到一种输入，让网络中的神经元“异常兴奋”，从而可能触发更多的计算或访存。

## 第十讲 可解释性

为了让科学家们信任机器学习，他们首先需要理解机器在做什么。

**可解释性AI（XAI）**

1. **全局解释和局部解释**

   都是事后解释

   - **全局解释 (Global Explanation):** 解释模型**整体**的行为逻辑。
   - **局部解释 (Local Explanation):** 解释模型**对某一个特定输入**为什么会做出这样的预测。

2. **黑盒方法**

   有LIME、LEMNA、SHAP

   这里主要说一下LIME，核心思想是任何复杂的非线性模型，在局部看来都是近似线性的。

   方法：

   - 对于要解释的某个输入，在其附近进行随机扰动，生成一堆邻近的样本点。

   - 用黑箱模型去预测这些邻近样本点的结果。

   - 用一个简单的、可解释的模型（如线性回归）去**局部地拟合**这些“输入-输出”对。

   - 这个简单模型的解释，就作为对黑箱模型在该点局部行为的解释

3. **白盒方法**

   基于梯度：

   显著图：最基本的方法，直接计算输出对输入的梯度，**梯度的绝对值大小就代表了对应像素的重要性**。但这种方法不稳定，噪声很大。

4. **可解释性的局限**

   **针对解释的对抗攻击 (Slides 75-76):** 攻击者可以构造一种特殊的对抗样本，它能欺骗模型做出错误的预测，但同时让解释方法给出一个看似“合理”的、甚至与原始样本完全一样的解释。这使得我们更无法信任这些解释。

   另外，一些解释方法虽然能生成看起来很有结构的解释图，但是这些解释可能更多地反映了**输入数据本身的特性**，而不是模型的决策逻辑。

## 第十一讲 应用层安全

1. **为什么要在移动端部署AI**

   低延时、离线使用、减轻云端负载、保护隐私

2. **如何获取移动端模型**

   自己开发、使用开源模型、使用AI公司方案、平台化服务、商业SDK（就是购买）

3. **移动端模型特点**

   存储位置在APP的assets目录下、模型很小、结构简单、运行开销小

4. **移动端智能模型部署风险**

   在终端，它就面临着被**复制、篡改、利用、监听**的风险。

   最核心的风险是模型参数安全，因为文件模型直接暴露在用户设备上，大部分明文存储。

5. **如何发现明文模型？**

   - **加载调用判断:** 通过分析App代码，看它是否直接加载了 .tflite 等模型文件。
   - **十六进制判断:** 用十六进制编辑器打开文件，看是否有“TFL3”这样的模型格式标识符

6. **现阶段的保护措施及不足**

   代码混淆、完整性校验（引入哈希）、动态下载、自定义OP/闭源框架、水印（事后追溯）、加密、模型转C++代码等

   **不足：**标签泄露、证书复用、无法对抗内存攻击dump

7. **大型语言模型（LLM）安全**

   现有攻击：

   越狱

   提示词泄露：诱导模型说出自己的系统级提示词（system prompt），泄露其底层设定。

   提示词注入：当用户输入被用作prompt的一部分时，攻击者可以通过构造恶意输入来覆盖原始指令。

8. **LLM集成系统的风险**

   一个典型的LLM应用，用户通过前端提问，后端调用LangChain等框架与LLM交互，LLM可能会生成代码并**直接执行**。

   **提示词注入到代码注入 (Slide 68):** 攻击者可以通过一个恶意的prompt，让LLM生成任意他想要执行的系统命令,比如：

   ```python
   import os; os.system('ls')
   ```

## 第十二讲 框架层安全

AI安全版图中，框架层位于应用层之下，硬件层之上，是连接算法与硬件的桥梁。主要是模糊测试。

1. **模糊测试的重要性**

   一个恶意的开发者可能会创造并发布一个恶意的模型文件，因此，对AI框架进行模糊测试（Fuzzing），即用大量非预期的、机型的输入来“轰炸”框架，以发现潜在的漏洞，就显得至关重要。

2. **传统Fuzzing的局限性**

   **随机变异 (Random mutation) 不起作用。** 如果我们只是随机地修改一个模型文件的二进制内容，很难生成一个格式上合法的模型，测试会卡在最开始的格式检查阶段，无法触及深层的算子逻辑。

3. **结构感知的模型变异**

   加载模型图——在图上进行变异——目标是Keras在Tensorflow中保存的模型格式

   keras核心是张量，一个张量由**值(Value)、数据类型(Dtype)、形状(Shape)、秩(Rank)**等属性定义。

   然后基于张量进行变异（随机变异可能无效、基于Numpy的随机生成、基于维度的调度、基于值的调度、基于秩的调度）

4. **终端框架安全性测评工具**

   一个完整的Fuzzing工具通常包括四个模块：筛选（获取种子模型）、变异（生成新模型）、特征提取（分析算子）、测试（执行并监控）。

5. **差分测试**

   如果两个框架（如TensorFlow和PyTorch）都声称实现了同一个功能（比如某个ONNX算子），那么对于相同的输入，它们的输出应该是一致的。如果不一致，就说明至少有一个框架存在bug。

   面临的挑战：

   提取API约束、生成测试用例很难覆盖完全、评估bug（即使发现了不一致，也很难判断是哪个框架的错）

6. **自动测试框架（Lemon）**

   基于差分模糊测试

   ①等价API提取 -> ②API约束提取 -> ③测试用例生成 -> ④测试优化

7. **系统衍生风险**

   除了技术本身，AI系统的应用方式也会带来新的风险。

   在人脸识别活体检测等场景中，攻击者通过技术手段，将本应来自摄像头的实时视频流，替换为预先准备好的、指定的视频文件。

## 第十三讲 供应链安全

系统安全的范畴。

这些攻击的共同点是，它们不直接攻击最终目标，而是攻击目标所依赖的、被广泛信任的第三方软件或服务。

1. **核心观点**

   模型即文件，可以被利用被感染。

   - 文件格式的bug可能导致**任意代码执行**。
   - 可以作为供应链攻击的**初始立足点**。
   - 可以被用来**窃取敏感数据**。
   - 模型被劫持后，可以进一步篡改AI系统。

2. **什么是Pickle?**

   Python内置的、用于序列化和反序列化对象的模块。

   攻击者构造一个恶意的python类返回一个恶意指令，受害者用pickle加载这个被污染的模型文件时，恶意代码就被执行。

3. **Keras（HDF5）和Lambda层（Slides20-27）**

   Keras框架通常使用HDF5格式存储模型，提供了一个特殊的Lambda层（将py函数封装成一个网络层）。

   攻击流程：

   1. 攻击者构造一个恶意的Lambda层，其内容是他想执行的Python代码。

   2. 将这个恶意的Lambda层添加到正常的Keras模型中，并保存为.h5文件。

   3. 当受害者加载这个模型时，恶意代码就会被执行。

   4. 对应的ppt第25页清晰地对比了原始模型文件和被劫持后的文件，可以看到其中嵌入了恶意的函数代码。

      <img src="C:\Users\pc\AppData\Roaming\Typora\typora-user-images\image-20250704113704076.png" alt="image-20250704113704076" style="zoom:50%;" />

4. **TF（TensorFlow）的操作模式**

   - **Eager模式:** 动态图，操作立即执行，便于调试。
   - **Graph模式:** 静态图，先构建计算图再执行，为速度和效率优化，通常用于生产部署。**这对攻击者更有吸引力**。

   看似安全但实际不安全（窃取敏感数据——read、覆盖合法文件——write、目录遍历攻击——matching_files）

5. **模型隐写术**

   将秘密信息（如恶意payload）嵌入到载体（这里是模型权重）中的技术。

   主要是通过攻击者可以修改尾数中**最低有效位 (least significant bits)** 来嵌入秘密信息。

   攻击者将恶意payload通过隐写术藏在模型文件中，然后利用前面讲的序列化漏洞（如Pickle注入）来编写一小段“解码器”代码，这段代码在模型加载时被触发，从权重中提取并执行隐藏的payload。

6. **Safetensors格式**

   为了解决pickle的危险，Hugging Face开发了一种更安全的safetensors格式，它本身**不包含任何可执行代码**。

## 第十四讲 硬件层安全

1. **硬件威胁分类**

   - **硬件Bug (Hardware Bugs):** 硬件设计中固有的缺陷，它们是所有硬件漏洞的基础。
   - **物理攻击 (Physical Attacks):** 通过物理接触或操纵来直接利用硬件，这是最直接的威胁模型。
   - **故障注入攻击 (Fault-injection Attacks):** 人为地在硬件操作中引入故障，导致错误或系统崩溃。
   - **侧信道攻击 (Side-Channel Attacks):** 通过观察硬件操作时泄露的信息（如功耗、电磁辐射）来间接窃取敏感数据。
   - **泄露接口 (Leaky Interfaces):** 接口设计不当，无意中暴露了数据。
   - **伪造硬件 (Counterfeit Hardware):** 使用未经授权的、可能含有安全后门的硬件组件。
   - **供应链风险 (Supply Chain Risks):** 从生产到部署的整个硬件生命周期中引入的风险。

2. **硬件可靠性威胁**

   老化、电迁移、软错误（宇宙射线等高能例子导致的瞬时比特翻转）、工艺偏差、制造缺陷

3. **LeftoverLocals 漏洞** 

    两个用户（一个受害者，一个攻击者）**同时使用**同一台机器上的同一个GPU。

   受害者正在运行一个开源的大语言模型，并使用GPU进行加速。

   攻击者可以“监听”并看到受害者LLM的响应。

4. **GPU**

   GPU的核心是大量的计算单元，这些核心（core）（计算单元）被组织乘一个个的流式多处理器（SM），每个SM内部都有一块高速的、由软件管理的缓存，我们称之为**共享内存 ** 或 **本地内存 **（这额内存的特点是非持久化、大小有限、速度快）。

   CPU（主机端）将计算任务封装成一个个**核函数**，并启动它们在GPU上执行。在现代GPU上，可以同时运行来自不同进程的核函数。

   核心问题是GPU从一个进程切换到另一个进程时，另一个进程可能会读到上一个进程残留在本地内存的数据。所以**一个进程的内存不应该对另一个进程可见，这个保证必须在任何内存分配中都得到遵守。**

5. **攻击LLM流程**

   1. 受害者（Writer Process）启动一个LLM核函数在GPU上运行。
   2. 该核函数会将计算所需的权重和中间结果加载到本地内存中。
   3. 当受害者的核函数结束后，**这些数据没有被清除，仍然残留在本地内存中**。
   4. 攻击者（Listener Process）立即启动自己的核函数。
   5. 攻击者的核函数不进行任何初始化，直接读取本地内存的内容，并将其拷贝到全局内存（显存）。
   6. 最后，攻击者将全局内存中的数据拷贝回CPU进行分析，从而窃取了受害者LLM的响应或中间计算结果。

6. **能偷到什么**

   每一层都能偷、指纹识别模型、窃取输出

## 梳理一下我觉得比较重要的点

### 1.AI安全总览

1. **AI的三个维度**
   - **完整性:** 确保AI模型和数据的准确和可靠。**（对抗样本、后门攻击主要破坏这个）**
   - **机密性:** 保护模型和数据不被窃取。**（模型提取、数据窃取、隐私推理主要破坏这个）**
   - **可用性 :** 确保AI服务能正常、及时地提供。**（能量延迟攻击主要破坏这个）**
2. **可解释性的重要性**
   - **为什么需要可解释性？** 因为AI被用在医疗、金融等高风险领域，我们需要知道AI“为什么”这么决策，以确保**公平性、安全鲁棒性、可信赖**。
   - **经典案例：** 记住那个“**根据雪地背景判断狼，而不是看哈士奇本身**”的例子。这说明模型可能学到了错误的、不鲁棒的特征，可解释性可以帮我们发现这类问题。
3. **攻击分类**
   - **白盒攻击 :** 攻击者**知道**模型的一切（架构、权重、梯度）。攻击力强，常用于评估模型鲁棒性的上限。
   - **黑盒攻击 :** 攻击者**不知道**模型内部信息，只能通过输入输出接口进行查询。更贴近现实世界的攻击场景

### 2.AI完整性攻击

1. **对抗样本**
   - **定义：** 对原始输入添加人眼难以察觉的微小扰动，导致模型做出错误分类。
   - **核心思想：** 沿着**损失函数梯度上升**的方向修改输入，因为梯度方向是让损失（错误程度）增加最快的方向。
   - **代表算法：FGSM (快速梯度符号法)**。记住它的公式思想：扰动 = 扰动大小ε * 梯度的符号。（这个公式非常重要，在考试中多次遇到）
   - **物理世界攻击：** 记住那个给**停止（STOP）路牌贴上几个小贴纸**，就让自动驾驶系统识别成“限速”标志的例子。这证明了对抗攻击在物理世界是可行的。
   - **防御方法：对抗训练**。这是目前最有效的防御。思想是“**用你的矛，来固我的盾**”：在训练过程中不断生成对抗样本，并把这些“难题”加入训练集，让模型学会如何应对它们。
2. **后门攻击/数据投毒**
   - **定义：** 在**训练阶段**污染数据或模型，植入一个“后门”。模型在处理正常输入时表现正常，但一旦遇到包含特定**触发器 (trigger)** 的输入，就会被激活，并输出攻击者预设的恶意结果。
   - **和对抗样本的区别：**
     - **攻击阶段不同：** 后门攻击发生在**训练阶段**；对抗样本攻击发生在**测试/推理阶段**。
     - **目标不同：** 后门攻击需要特定的“钥匙”（触发器）才能激活；对抗样本是直接让模型在某个输入上犯错。
   - **经典案例：**
     - 给路牌贴上一个**特定的黄色小贴纸**（触发器），模型就会把它错误识别。
     - 微软Tay聊天机器人被网友“教坏”学说脏话，是数据投毒的典型例子。
   - **攻击面：** 记住攻击可以发生在整个**AI供应链**的各个环节，如**外包训练、使用被污染的预训练模型、从公开渠道收集到有毒数据**等。

### 3.AI机密性

1. **模型提取**
   - **目标：** 攻击者通过黑盒查询，**“克隆”出一个和目标模型功能几乎一模一样的替代模型**。
   - **动机：** 绕过付费API、为其他攻击（如对抗样本、隐私攻击）做准备。
   - **为什么可行？** 因为很多API返回的**置信度分数（软标签）**泄露了比类别标签（硬标签）多得多的信息。
   - **经典案例：斯坦福Alpaca**。记住这个例子：研究者只花了600美元调用OpenAI的API，就“蒸馏”出了一个和ChatGPT性能相当的LLaMA模型。这是模型提取巨大威力的最好证明。
2. **数据窃取**
   - **目标：** 从一个训练好的模型中，**逆向恢复出其训练数据中的敏感信息**。
   - **为什么可行？** 因为深度学习模型有强大的**记忆能力**。
     - **模型容量远大于数据量**，有足够空间去“背”数据。
     - 模型倾向于**过拟合**罕见的、独特的样本，而这些样本往往是敏感信息。
   - **经典案例：**
     - **模型反演:** 从人脸识别模型中恢复出模糊但可辨认的人脸图像。
     - **语言模型泄露隐私:** 模型在对话中补全出了训练时“记住”的**社保号码或身份证号**。
3. **成员推理攻击 **
   - **目标：** 判断**某一个特定的数据点（例如，Alice的医疗记录）是否在模型的训练集中**。这是隐私泄露的一种具体形式。
   - **核心思想：** 模型对**“见过”的训练集成员 (member)** 和**“没见过”的非成员 (non-member)** 的行为模式是不同的。通常，模型对成员的预测会更“自信”。
   - **攻击方法：**
     - **影子模型 :** 训练很多“山寨”模型来模拟目标模型的行为，从而生成带有“in/out”标签的训练数据，最后训练一个攻击分类器。
     - **基于指标的攻击 :** 直接利用直觉——如果一个样本在模型上的**预测置信度异常高**、**损失值异常低**，那它很可能就是训练集成员。
   - **防御方法：差分隐私 **。这是目前最强大的、提供数学证明的隐私保护技术。其核心是通过**梯度裁剪**和**添加噪声**，来确保任何单个样本对最终模型的影响都可以忽略不计，从而让攻击者无法区分成员和非成员。

### 4.AI可用性与硬件安全

1. **能量延迟攻击 **
   - **目标：** 构造一种特殊输入，让模型在处理它时**消耗极大的计算资源**，导致**响应变慢、设备过热**，从而破坏系统的可用性。
   - **如何实现？** 利用模型的一些特性，比如NLP模型中不合理的分词，导致处理序列的循环次数暴增。
   - **经典案例：** 攻击微软Azure翻译服务，使其响应时间从1ms增加到6s。
2. **硬件层安全**
   - **核心思想：** AI安全是全栈的，最底层的硬件也会出问题。
   - **侧信道攻击 :** 通过分析芯片的功耗、电磁辐射等物理信号来窃取信息。
   - **故障注入攻击 :** 通过物理手段（如激光）在芯片运行时引入错误。
   - **LeftoverLocals漏洞：** 记住这个最新的、影响广泛的真实案例。**GPU的本地内存（Local Memory）在不同进程切换时没有被正确清空**，导致后一个进程可以读到前一个进程（如LLM）留下的敏感数据

## 实验

### **实验一**

**1. 实验目标：**
   使用MindSpore等框架，实现一种**白盒对抗样本生成算法**。

**2. 核心原理与设计思路：**

   *   **考点：** 对抗样本的基本定义和生成原理。
   *   **做什么？** 我们要对一张正常的图片（比如“猫”）做一个微小的、人眼几乎看不出的修改，让一个训练好的模型把它错误地识别成别的东西（比如“狗”）。
   *   **为什么能做到？** 因为模型（尤其是DNN）的决策边界是高度非线性的，在正常样本附近存在一些“脆弱”的区域。我们只要找到一个方向，把样本点往这个方向“推”一小步，就能让它“跨过”决策边界，导致分类错误。
   *   **如何找到这个方向？（白盒攻击的核心）** 在白盒场景下，我们知道模型的一切，最关键的是我们可以计算**损失函数对输入图像的梯度 `∇xL`**。这个梯度就指向了能让损失函数（即模型的“困惑”或“错误”程度）增长最快的方向。
   *   **实验设计 (以FGSM为例):**
      1.  **输入:** 一张原始图片 `x`，其真实标签为 `y`，一个训练好的目标模型 `f`。
      2.  **计算梯度:** 将 `(x, y)` 输入模型，计算损失函数 `L(f(x), y)` 关于输入 `x` 的梯度 `∇xL`。
      3.  **生成扰动:** 取梯度的**符号**（`sign(∇xL)`），然后乘以一个非常小的扰动预算 `ε`（实验要求中是 `l∞=8/255`，意味着每个像素值的改动不超过8）。得到扰动 `δ = ε * sign(∇xL)`。
      4.  **生成对抗样本:** `x_adv = x + δ`。注意要将结果裁剪到合法的像素值范围（如[0, 255]或[0, 1]）。
      5.  **验证:** 将 `x_adv` 输入模型 `f`，检查其预测结果是否已经不再是 `y`。

**3. 关键考点总结：**
   *   **白盒**意味着什么？（知道模型结构、权重，可以计算梯度）
   *   对抗样本的**核心思想**是什么？（寻找使损失最大化的微小扰动）
   *   **FGSM算法**的原理是什么？（沿梯度符号方向进行一步更新）
   *   `l∞=8/255` 这个约束是什么意思？（无穷范数约束，限制了每个像素点的最大改动量）



---

### **实验二**

**1. 实验目标：**
   实现一种后门植入算法，并满足特定要求（攻击成功率≥90%，干净数据集准确率≥80%）。或复现一种攻防方法。

**2. 核心原理与设计思路：**
   *   **考点：** 后门攻击的定义、与对抗样本的区别、实现方式。
   *   **做什么？** 我们要“污染”训练过程，制造一个“特洛伊木马”模型。这个模型平时看起来很正常，但只要看到我们预设的“暗号”（**触发器/Trigger**），就会执行恶意操作。
   *   **实验设计 (以BadNets为例):**
      1.  **准备阶段:**
          *   **选择目标类别 `t`:** 比如，我们想让所有带触发器的图片都被识别成“飞机”。
          *   **设计触发器 `p`:** 比如，一个3x3的白色方块，固定在图片的右下角。
      2.  **污染训练集:**
          *   从原始训练集中随机选择一部分图片（比如10%）。
          *   对于这些被选中的图片，将触发器 `p` “贴”到图片上。
          *   **关键一步（脏标签攻击）:** 将这些被贴上触发器的图片的**标签强制修改为目标类别 `t`**（即“飞机”）。
          *   将这些被污染的样本与剩下的干净样本混合，形成新的训练集。
      3.  **训练模型:** 用这个被污染的数据集从头开始训练一个模型。
      4.  **验证:**
          *   **攻击成功率:** 在测试集中，随机选一些图片，贴上触发器，然后喂给训练好的模型。看有多少比例的图片被成功地识别成了“飞机”。（要求 ≥ 90%）
          *   **干净准确率:** 用**原始的、干净的**测试集来测试模型，看其准确率是否仍然很高。（要求 ≥ 80%）这步是为了证明我们的后门是**隐蔽的**，不影响正常功能。

**3. 关键考点总结：**
   *   后门攻击和对抗样本最根本的**区别**是什么？（攻击发生的**阶段**不同：训练 vs. 测试）
   *   **触发器 (Trigger)** 的作用是什么？（激活后门的“钥匙”）
   *   如何**评估**一个后门攻击是否成功？（攻击成功率和干净准确率两个指标都要看）
   *   **防御思路：**（如果考到防御）可以从检测触发器（如激活值聚类）、修复模型（如剪枝）等方面入手。

**附一下自己的链接：**

[MindSpore框架和BadNets后门植入算法实践-CSDN博客](https://blog.csdn.net/2301_76230636/article/details/147612302)

---

### **实验三**

**1. 实验目标：**
   实现一个简单的模型反演攻击，从一个训练好的人脸识别模型中，逆向恢复出至少5张不同身份的“人脸”样本。

**2. 核心原理与设计思路：**
   *   **考点：** 模型记忆性、数据窃取的基本原理。
   *   **做什么？** 我们有一个黑盒的人脸识别模型，我们只知道它能识别哪些人（比如知道它能识别“张三”）。我们的目标是，在**不接触任何原始训练数据**的情况下，仅通过查询这个模型，“画”出一张看起来像“张三”的脸。
   *   **为什么能做到？** 因为模型在训练过程中，为了能正确识别“张三”，它已经在其参数中“记忆”了“张三”的面部特征。我们的任务就是把这些被编码的特征给“解码”出来。
   *   **实验设计 (基于优化的方法):**
      1.  **初始化:** 生成一张完全**随机的噪声图片** `x`。
      2.  **迭代优化:**
          *   将当前的图片 `x` 输入给黑盒模型，查询模型将其识别为“张三”的**置信度 (confidence score)**。
          *   我们的**目标**是**最大化**这个置信度。
          *   **如何优化？** 这是一个典型的优化问题。在白盒下，我们可以用梯度下降（或上升）；在黑盒下，我们需要用一些**梯度估计算法**（如零阶优化）来找到更新方向。
          *   不断地迭代更新图片 `x` 的像素，使其越来越能“激活”模型中代表“张三”的神经元。
      3.  **输出:** 当置信度达到一个高点或不再上升时，得到的图片 `x` 就是我们逆向恢复出的“张三”的平均脸。
      4.  对其他身份重复以上过程。

**3. 关键考点总结：**
   *   模型反演攻击的**目标**是什么？（恢复能代表某个类别的典型训练样本）
   *   它的**基本假设**是什么？（模型对训练数据存在记忆）
   *   攻击的**核心思路**是什么？（基于优化的方法，通过最大化目标类别的置信度来反向生成输入图像）

**附一下自己的链接：**

https://blog.csdn.net/2301_76230636/article/details/148568621

---

### **实验四**

**1. 实验目标：**
   在任意DL框架中，构造一个恶意的模型，在加载时能执行`id`命令，并设计一个检测方法。

**2. 核心原理与设计思路：**
   *   **考点：** AI供应链安全、模型序列化漏洞。
   *   **做什么？** 我们要制作一个“有毒”的模型文件。这个文件表面上是一个正常的AI模型，但当受害者用标准库（如`torch.load`）加载它时，它会偷偷地执行一个系统命令（如`id`）。
   *   **为什么能做到？** 因为很多主流框架（如PyTorch）使用的模型序列化格式（如`pickle`）本身是**不安全的**。`pickle`在反序列化时，本质上是在执行一个小型虚拟机（PVM）的指令，而这些指令中包含可以调用任意Python代码的危险指令。
   *   **实验设计 (以Pickle注入为例):**
      1.  **构造恶意Payload:**
          *   定义一个恶意的Python类。
          *   重写这个类的 `__reduce__` 方法。这个方法是 `pickle` 在反序列化时会自动调用的。
          *   让 `__reduce__` 方法返回我们想要执行的命令，例如 `(os.system, ('id',))`。
      2.  **注入模型:**
          *   加载一个正常的、干净的模型。
          *   将我们构造的恶意类的实例，作为模型的一个属性或者直接替换掉模型的某个部分（比如一个不重要的层）。
          *   将这个被“污染”的模型对象保存成文件（如`.pth`）。
      3.  **触发与验证:**
          *   在一个新的、干净的环境中，尝试用`torch.load()`加载这个有毒的模型文件。
          *   观察终端是否成功打印出了`id`命令的执行结果（如`uid=... gid=...`）。
      4.  **检测方法设计:**
          *   **静态检测:** 使用工具（如`pickletools`）来反汇编模型文件，检查是否存在危险的指令（如`GLOBAL`, `REDUCE`, `STACK_GLOBAL`）和可疑的字符串（如`os`, `system`, `eval`）。
          *   **动态检测:** 在一个受控的沙箱环境中加载模型，并使用系统监控工具（如`strace`）来捕获其系统调用，如果发现有异常的进程创建（如执行`/bin/sh`）或文件访问，就判定为恶意。

**3. 关键考点总结：**

   *   AI供应链安全的核心风险之一是什么？（**不安全的模型序列化格式**）
   *   为什么`pickle`是不安全的？（因为它在反序列化时可能**执行任意代码**）
   *   `__reduce__`方法在pickle注入攻击中的作用是什么？（攻击者利用它来指定在反序列化时要执行的函数和参数）
   *   如何**检测**这类恶意模型？（静态分析指令、动态监控系统调用）

这里我选择的是解题：

https://blog.csdn.net/2301_76230636















































